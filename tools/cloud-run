#!/usr/bin/env python3
"""cloud-run — Remote experiment execution for orchestration-kit.

Usage:
  tools/cloud-run <command> --spec <spec-file> [options]    # Run experiment remotely
  tools/cloud-run status [run-id]                           # Check run status
  tools/cloud-run pull <run-id> [--output-dir <dir>]        # Download results
  tools/cloud-run ls                                        # List tracked runs
  tools/cloud-run gc                                        # Clean up orphaned resources
  tools/cloud-run terminate <run-id>                        # Force-terminate a run
  tools/cloud-run reap [--dry-run] [--hard-ceiling N]       # Terminate expired instances
"""

from __future__ import annotations

import argparse
import json
import os
import sys
from pathlib import Path

# Add tools/ to path so we can import cloud module
sys.path.insert(0, str(Path(__file__).resolve().parent))

import subprocess

from cloud import remote, s3
from cloud.config import DEFAULT_MAX_HOURS, RUNPOD_DEFAULT_DATACENTER, ECR_REPO_URI, AWS_REGION
from cloud.preflight import check_spec
from cloud.remote import DuplicateSpecError
from cloud import state as project_state
from cloud.reaper import reap as reap_instances


def cmd_run(args):
    """Execute an experiment on a remote cloud instance."""
    spec_file = args.spec
    command = args.command

    # Run preflight to determine backend and instance type (unless overridden)
    backend_name = args.backend
    instance_type = args.instance_type
    use_spot = not args.on_demand
    preflight_gpu_mode = False

    if spec_file and (not backend_name or not instance_type):
        pf = check_spec(spec_file)
        preflight_gpu_mode = pf.get("gpu_mode", False)
        if not backend_name:
            if pf["recommendation"] == "local":
                print(f"Preflight says local execution is sufficient: {pf['reason']}")
                if not args.force:
                    print("Use --force to run remotely anyway.")
                    sys.exit(0)
                backend_name = "aws"
            else:
                backend_name = pf["backend"]
        if not instance_type:
            instance_type = pf.get("instance_type", "c7a.8xlarge")
        if "use_spot" in pf and pf["recommendation"] != "local":
            use_spot = pf["use_spot"] and not args.on_demand

    # GPU mode: explicit --gpu flag OR auto-detected from preflight (model_type=pytorch)
    gpu_mode = getattr(args, "gpu", False) or preflight_gpu_mode
    if preflight_gpu_mode and not getattr(args, "gpu", False):
        print(f"Auto-detected GPU mode from spec (model_type=pytorch)")
    if gpu_mode:
        backend_name = "aws"
        if not instance_type or instance_type in ("c7a.8xlarge", "g5.xlarge"):
            instance_type = "g5.xlarge"

    # Defaults if still not set
    backend_name = backend_name or "aws"
    instance_type = instance_type or "c7a.8xlarge"

    # Resolve project root
    project_root = args.project_root or os.environ.get("PROJECT_ROOT", os.getcwd())

    # Parse data dirs
    data_dirs = args.data_dirs.split(",") if args.data_dirs else None

    # Parse env vars
    env_vars = {}
    if args.env:
        for e in args.env:
            k, _, v = e.partition("=")
            env_vars[k] = v

    # --- Pre-launch: verify Docker image exists in ECR (skip in GPU mode) ---
    if backend_name == "aws" and ECR_REPO_URI and not args.dry_run and not gpu_mode:
        _tag = getattr(args, "image_tag", None) or "latest"
        _repo_name = ECR_REPO_URI.split("/")[-1]  # e.g. "mbo-dl" from full URI
        try:
            _check = subprocess.run(
                ["aws", "ecr", "batch-get-image",
                 "--repository-name", _repo_name,
                 "--image-ids", f"imageTag={_tag}",
                 "--region", AWS_REGION,
                 "--query", "images[0].imageId.imageTag",
                 "--output", "text"],
                capture_output=True, text=True, timeout=15,
            )
            if _check.returncode != 0 or _check.stdout.strip() in ("None", ""):
                print(f"ERROR: Docker image {_repo_name}:{_tag} not found in ECR.", file=sys.stderr)
                print(f"  Available tags: run 'aws ecr list-images --repository-name {_repo_name} --region {AWS_REGION}'", file=sys.stderr)
                print(f"  Build and push: 'cloud-run build --push' or 'tools/docker-build build python --push'", file=sys.stderr)
                sys.exit(1)
            print(f"ECR image verified: {_repo_name}:{_tag}")
        except subprocess.TimeoutExpired:
            print("WARNING: ECR image check timed out, proceeding anyway", file=sys.stderr)

    # Instantiate backend (deferred for dry-run — no SDK needed)
    backend = None if args.dry_run else _get_backend(backend_name)

    try:
        result = remote.run(
            command=command,
            backend=backend,
            backend_name=backend_name,
            project_root=project_root,
            spec_file=spec_file,
            instance_type=instance_type,
            data_dirs=data_dirs,
            sync_back=args.sync_back,
            local_results_dir=args.output_dir,
            use_spot=use_spot,
            max_hours=args.max_hours,
            detach=args.detach,
            dry_run=args.dry_run,
            env_vars=env_vars,
            network_volume_id=getattr(args, "volume", None),
            allow_duplicate=args.allow_duplicate,
            image_tag=getattr(args, "image_tag", None),
            gpu_mode=gpu_mode,
        )
    except DuplicateSpecError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        sys.exit(1)

    if args.json_output:
        print(json.dumps(result, indent=2))
    elif args.dry_run:
        print(f"\n[DRY RUN] Would execute on {backend_name} {instance_type}:")
        print(f"  Command:  {command}")
        print(f"  Data:     {data_dirs}")
        print(f"  Max hours: {args.max_hours}")
        print(f"  Spot:     {use_spot}")
        print(f"  Run ID:   {result['run_id']}")
    elif not args.detach:
        ec = result.get("exit_code", "?")
        status = result.get("status", "unknown")
        elapsed = result.get("elapsed_seconds", 0)
        print(f"\nDone. Status: {status}, exit code: {ec}, elapsed: {elapsed//60}m{elapsed%60}s")
        if result.get("local_results_dir"):
            print(f"Results: {result['local_results_dir']}")


def cmd_status(args):
    """Check the status of a cloud run."""
    project_root = getattr(args, "project_root", None) or os.environ.get("PROJECT_ROOT")
    if args.run_id:
        result = remote.poll_status(args.run_id)
        _print_run(result)
        # Verify against project-local state and clean stale entries
        if project_root:
            local = project_state.get_run(project_root, args.run_id)
            if local and result["status"] in ("completed", "failed", "terminated", "error"):
                project_state.remove_run(project_root, args.run_id)
    else:
        # Show status of most recent run
        runs = remote.list_runs()
        if not runs:
            print("No tracked runs.")
            return
        result = remote.poll_status(runs[0]["run_id"])
        _print_run(result)


def cmd_pull(args):
    """Download results from a completed run."""
    local_dir = remote.pull_results(args.run_id, args.output_dir)
    print(f"Results downloaded to: {local_dir}")


def cmd_ls(args):
    """List all tracked cloud runs (merges global + project-local state)."""
    runs = remote.list_runs()

    # Merge project-local active runs not already in global state
    project_root = getattr(args, "project_root", None) or os.environ.get("PROJECT_ROOT")
    if project_root:
        global_ids = {r["run_id"] for r in runs}
        for local in project_state.list_active_runs(project_root):
            if local["run_id"] not in global_ids:
                runs.append({
                    "run_id": local["run_id"],
                    "backend": local.get("backend", "?"),
                    "status": "active(local)",
                    "instance_type": local.get("instance_type", ""),
                    "command": "",
                })

    if not runs:
        print("No tracked runs.")
        return

    # Refresh status of running ones
    for r in runs:
        if r["status"] in ("running", "provisioning"):
            r = remote.poll_status(r["run_id"])

    fmt = "{:<36}  {:<12}  {:<6}  {:<14}  {:<8}  {}"
    print(fmt.format("RUN_ID", "BACKEND", "CODE", "STATUS", "HOURS", "COMMAND"))
    print("-" * 104)
    for r in runs:
        ec = str(r.get("exit_code", "-"))
        hrs = ""
        if r.get("elapsed_seconds"):
            hrs = f"{r['elapsed_seconds']/3600:.1f}"
        cmd = (r.get("command", "")[:40] + "...") if len(r.get("command", "")) > 40 else r.get("command", "")
        print(fmt.format(
            r["run_id"], r.get("backend", "?"), ec,
            r.get("status", "?"), hrs, cmd,
        ))


def cmd_gc(args):
    """Garbage-collect orphaned cloud resources."""
    for backend_name in ("aws", "runpod"):
        try:
            backend = _get_backend(backend_name)
            cleaned = backend.gc()
            for item in cleaned:
                print(f"  Cleaned: {item}")
            if not cleaned:
                print(f"  {backend_name}: no orphaned resources")
        except (ImportError, Exception) as e:
            print(f"  {backend_name}: skipped ({e})")


def cmd_terminate(args):
    """Force-terminate a running cloud instance."""
    state = remote._load_state(args.run_id)
    backend = _get_backend(state.get("backend", "aws"))
    remote.terminate_run(args.run_id, backend)


def cmd_reap(args):
    """Reap expired cloud instances past their lease or hard ceiling."""
    backend = _get_backend(args.backend)
    project_root = args.project_root or os.environ.get("PROJECT_ROOT")
    actions = reap_instances(
        backend,
        dry_run=args.dry_run,
        hard_ceiling_hours=args.hard_ceiling,
        project_root=project_root,
    )
    if not actions:
        print("No instances to reap.")
        return
    for a in actions:
        print(f"  {a['action']}: {a['instance_id']}  run={a['run_id']}  age={a['age_hours']:.1f}h  reason={a['reason']}")
    print(f"\nTotal: {len(actions)} instance(s) {'would be reaped' if args.dry_run else 'reaped'}.")


def cmd_volume_create(args):
    """Create a RunPod network volume."""
    backend = _get_backend("runpod")
    vol_id = backend.create_network_volume(args.name, args.size, args.datacenter)
    print(f"Created volume: {vol_id} ({args.name}, {args.size}GB, {args.datacenter})")


def cmd_volume_destroy(args):
    """Destroy a RunPod network volume."""
    backend = _get_backend("runpod")
    backend.destroy_volume(args.volume_id)
    print(f"Destroyed volume: {args.volume_id}")


def cmd_volume_sync(args):
    """Sync a file or directory to a RunPod network volume via S3 API."""
    backend = _get_backend("runpod")
    backend.sync_to_volume(args.volume_id, args.source, args.datacenter)
    print(f"Synced {args.source} → volume {args.volume_id}")


def cmd_volume_ls(args):
    """List RunPod network volumes."""
    backend = _get_backend("runpod")
    volumes = backend.list_volumes()
    if not volumes:
        print("No network volumes.")
        return
    fmt = "{:<14}  {:<24}  {:>6}  {:<10}"
    print(fmt.format("ID", "NAME", "SIZE", "DATACENTER"))
    print("-" * 60)
    for v in volumes:
        print(fmt.format(
            v.get("id", "?"), v.get("name", "?"),
            f"{v.get('size', '?')}GB", v.get("dataCenterId", "?"),
        ))


# ---------------------------------------------------------------------------
# Docker build / push / snapshot commands
# ---------------------------------------------------------------------------

def _resolve_image_tag(tag: str | None) -> str:
    """Resolve image tag — defaults to short git SHA."""
    if tag:
        return tag
    try:
        result = subprocess.run(
            ["git", "rev-parse", "--short", "HEAD"],
            capture_output=True, text=True, timeout=10,
        )
        if result.returncode == 0:
            return result.stdout.strip()
    except Exception:
        pass
    return "latest"


def _resolve_ecr_repo() -> str:
    """Return ECR repo URI or exit with error."""
    if not ECR_REPO_URI:
        print("ERROR: CLOUD_RUN_ECR_REPO_URI not set. Run:", file=sys.stderr)
        print("  export CLOUD_RUN_ECR_REPO_URI=<account>.dkr.ecr.<region>.amazonaws.com/<repo>", file=sys.stderr)
        sys.exit(1)
    return ECR_REPO_URI


def cmd_build(args):
    """Build the Docker image for remote execution."""
    tag = _resolve_image_tag(args.tag)
    ecr_repo = _resolve_ecr_repo()
    full_tag = f"{ecr_repo}:{tag}"
    latest_tag = f"{ecr_repo}:latest"

    project_root = args.project_root or os.environ.get("PROJECT_ROOT", os.getcwd())
    platform = args.platform or "linux/amd64"

    # Always dual-tag: requested tag + latest
    cmd = ["docker", "buildx", "build", "--platform", platform,
           "-t", full_tag, "-t", latest_tag]
    if args.no_cache:
        cmd.append("--no-cache")
    if args.push:
        cmd.append("--push")
    cmd.append(project_root)

    print(f"Building: {full_tag}")
    if tag != "latest":
        print(f"  Also tagging as: {latest_tag}")
    print(f"  Platform: {platform}")
    print(f"  Context:  {project_root}")
    result = subprocess.run(cmd, cwd=project_root)
    if result.returncode != 0:
        print(f"Build failed with exit code {result.returncode}", file=sys.stderr)
        sys.exit(result.returncode)

    print(f"\nImage built: {full_tag}")
    if not args.push:
        print(f"Push with: cloud-run push --tag {tag}")


def cmd_push(args):
    """Authenticate to ECR and push the Docker image."""
    tag = _resolve_image_tag(args.tag)
    ecr_repo = _resolve_ecr_repo()
    full_tag = f"{ecr_repo}:{tag}"
    latest_tag = f"{ecr_repo}:latest"

    # Extract registry hostname from repo URI
    registry = ecr_repo.split("/")[0]

    # ECR login
    print(f"Authenticating to ECR: {registry}")
    login_pw = subprocess.run(
        ["aws", "ecr", "get-login-password", "--region", AWS_REGION],
        capture_output=True, text=True, timeout=30,
    )
    if login_pw.returncode != 0:
        print(f"ECR login failed: {login_pw.stderr}", file=sys.stderr)
        sys.exit(1)

    docker_login = subprocess.run(
        ["docker", "login", "--username", "AWS", "--password-stdin", registry],
        input=login_pw.stdout, capture_output=True, text=True, timeout=30,
    )
    if docker_login.returncode != 0:
        print(f"Docker login failed: {docker_login.stderr}", file=sys.stderr)
        sys.exit(1)

    # Push the requested tag
    print(f"Pushing: {full_tag}")
    result = subprocess.run(["docker", "push", full_tag])
    if result.returncode != 0:
        print(f"Push failed with exit code {result.returncode}", file=sys.stderr)
        sys.exit(result.returncode)
    print(f"  Pushed: {full_tag}")

    # Always also push :latest so ec2-bootstrap default works
    if tag != "latest":
        print(f"Tagging and pushing: {latest_tag}")
        subprocess.run(["docker", "tag", full_tag, latest_tag], check=True)
        result = subprocess.run(["docker", "push", latest_tag])
        if result.returncode != 0:
            print(f"WARNING: Failed to push {latest_tag} (non-fatal)", file=sys.stderr)
        else:
            print(f"  Pushed: {latest_tag}")

    print(f"\nDone. Image available as {full_tag} and {latest_tag}")


def cmd_snapshot_list(args):
    """List EBS snapshots tagged with ManagedBy=cloud-run."""
    result = subprocess.run(
        [
            "aws", "ec2", "describe-snapshots",
            "--filters", "Name=tag:ManagedBy,Values=cloud-run",
            "--query", "Snapshots[*].{ID:SnapshotId,Size:VolumeSize,State:State,Desc:Description,Date:StartTime}",
            "--output", "table",
            "--region", AWS_REGION,
        ],
        timeout=30,
    )
    if result.returncode != 0:
        sys.exit(result.returncode)


def cmd_snapshot_create(args):
    """Print manual steps to create an EBS data snapshot."""
    print("EBS data snapshot creation (one-time manual process):")
    print()
    print("1. Launch a temporary EC2 instance with a 60GB gp3 volume")
    print("2. Attach and mount the volume:")
    print("     sudo mkfs.ext4 /dev/xvdf && sudo mount /dev/xvdf /data")
    print("3. Sync MBO data to the volume:")
    print("     aws s3 sync s3://kenoma-labs-research/DATA/ /data/")
    print("4. Unmount and create snapshot:")
    print("     sudo umount /data")
    print("     aws ec2 create-snapshot --volume-id <vol-id> \\")
    print("       --description 'MBO MES 2022 dataset' \\")
    print("       --tag-specifications 'ResourceType=snapshot,Tags=[{Key=ManagedBy,Value=cloud-run},{Key=Project,Value=orchestration-kit}]'")
    print("5. Set the snapshot ID:")
    print("     export CLOUD_RUN_EBS_SNAPSHOT_ID=snap-xxxxxxxxx")
    print("6. Terminate the temporary instance")


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------

def _get_backend(name: str):
    if name == "aws":
        from cloud.backends.aws import AWSBackend
        return AWSBackend()
    elif name == "runpod":
        from cloud.backends.runpod import RunPodBackend
        return RunPodBackend()
    else:
        raise ValueError(f"Unknown backend: {name}")


def _print_run(r: dict):
    print(f"Run ID:     {r['run_id']}")
    print(f"Status:     {r['status']}")
    print(f"Backend:    {r.get('backend', '?')} {r.get('instance_type', '')}")
    print(f"Instance:   {r.get('instance_id', '-')}")
    print(f"Command:    {r.get('command', '-')}")
    print(f"Started:    {r.get('started_at', '-')}")
    if r.get("finished_at"):
        print(f"Finished:   {r['finished_at']}")
    if r.get("exit_code") is not None:
        print(f"Exit code:  {r['exit_code']}")
    if r.get("elapsed_seconds"):
        print(f"Elapsed:    {r['elapsed_seconds']//60}m{r['elapsed_seconds']%60}s")
    if r.get("local_results_dir"):
        print(f"Results:    {r['local_results_dir']}")
    if r.get("error"):
        print(f"Error:      {r['error']}")


# ---------------------------------------------------------------------------
# Argument parser
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        prog="cloud-run",
        description="Remote experiment execution for orchestration-kit",
    )
    sub = parser.add_subparsers(dest="subcmd")

    # --- run (default when a command string is given) ---
    p_run = sub.add_parser("run", help="Execute experiment remotely")
    p_run.add_argument("command", help="Shell command to execute on remote instance")
    p_run.add_argument("--spec", help="Experiment spec file (for compute profile)")
    p_run.add_argument("--backend", choices=["aws", "runpod"], help="Force backend")
    p_run.add_argument("--instance-type", help="Override instance type")
    p_run.add_argument("--data-dirs", help="Comma-separated local dirs to upload")
    p_run.add_argument("--sync-back", default="results", help="Remote dir to pull results from")
    p_run.add_argument("--output-dir", help="Local dir for downloaded results")
    p_run.add_argument("--project-root", help="Project root to upload (default: cwd or $PROJECT_ROOT)")
    p_run.add_argument("--spot", action="store_true", default=True, help="Use spot instances (default)")
    p_run.add_argument("--on-demand", action="store_true", help="Use on-demand instances")
    p_run.add_argument("--max-hours", type=float, default=DEFAULT_MAX_HOURS, help=f"Auto-terminate after N hours (default: {DEFAULT_MAX_HOURS})")
    p_run.add_argument("--detach", action="store_true", help="Launch and return immediately")
    p_run.add_argument("--dry-run", action="store_true", help="Show what would happen")
    p_run.add_argument("--force", action="store_true", help="Run remotely even if preflight says local")
    p_run.add_argument("--env", action="append", help="Extra env vars (KEY=VALUE), repeatable")
    p_run.add_argument("--volume", help="RunPod network volume ID to mount at /workspace")
    p_run.add_argument("--allow-duplicate", action="store_true", help="Allow launching even if an instance with the same spec is running")
    p_run.add_argument("--image-tag", help="ECR image tag to use (default: git SHA)")
    p_run.add_argument("--gpu", action="store_true", help="Use GPU instance with PyTorch Deep Learning AMI (no Docker)")
    p_run.add_argument("--json", dest="json_output", action="store_true", help="JSON output")
    p_run.set_defaults(func=cmd_run)

    # --- status ---
    p_status = sub.add_parser("status", help="Check run status")
    p_status.add_argument("run_id", nargs="?", help="Run ID (default: most recent)")
    p_status.set_defaults(func=cmd_status)

    # --- pull ---
    p_pull = sub.add_parser("pull", help="Download results")
    p_pull.add_argument("run_id", help="Run ID")
    p_pull.add_argument("--output-dir", help="Local dir for results")
    p_pull.set_defaults(func=cmd_pull)

    # --- ls ---
    p_ls = sub.add_parser("ls", help="List tracked runs")
    p_ls.set_defaults(func=cmd_ls)

    # --- gc ---
    p_gc = sub.add_parser("gc", help="Garbage-collect orphaned resources")
    p_gc.set_defaults(func=cmd_gc)

    # --- terminate ---
    p_term = sub.add_parser("terminate", help="Force-terminate a run")
    p_term.add_argument("run_id", help="Run ID")
    p_term.set_defaults(func=cmd_terminate)

    # --- reap ---
    p_reap = sub.add_parser("reap", help="Terminate expired cloud instances")
    p_reap.add_argument("--dry-run", action="store_true", help="Show what would be reaped without terminating")
    p_reap.add_argument("--hard-ceiling", type=float, default=24, help="Hard ceiling in hours (default: 24)")
    p_reap.add_argument("--backend", default="aws", choices=["aws", "runpod"], help="Backend to reap (default: aws)")
    p_reap.add_argument("--project-root", help="Project root for cleaning local state")
    p_reap.set_defaults(func=cmd_reap)

    # --- volume ---
    p_vol = sub.add_parser("volume", help="Manage RunPod network volumes")
    vol_sub = p_vol.add_subparsers(dest="vol_cmd")

    p_vc = vol_sub.add_parser("create", help="Create a network volume")
    p_vc.add_argument("name", help="Volume name")
    p_vc.add_argument("--size", type=int, default=10, help="Size in GB (default: 10, min: 10)")
    p_vc.add_argument("--datacenter", default=RUNPOD_DEFAULT_DATACENTER, help=f"Datacenter (default: {RUNPOD_DEFAULT_DATACENTER})")
    p_vc.set_defaults(func=cmd_volume_create)

    p_vd = vol_sub.add_parser("destroy", help="Destroy a network volume")
    p_vd.add_argument("volume_id", help="Volume ID")
    p_vd.set_defaults(func=cmd_volume_destroy)

    p_vs = vol_sub.add_parser("sync", help="Sync file/dir to a volume via S3 API")
    p_vs.add_argument("volume_id", help="Volume ID")
    p_vs.add_argument("source", help="Local file or directory to upload")
    p_vs.add_argument("--datacenter", default=RUNPOD_DEFAULT_DATACENTER, help=f"Datacenter (default: {RUNPOD_DEFAULT_DATACENTER})")
    p_vs.set_defaults(func=cmd_volume_sync)

    p_vl = vol_sub.add_parser("ls", help="List network volumes")
    p_vl.set_defaults(func=cmd_volume_ls)

    # --- build ---
    p_build = sub.add_parser("build", help="Build Docker image for remote execution")
    p_build.add_argument("--tag", help="Image tag (default: git SHA)")
    p_build.add_argument("--platform", default="linux/amd64", help="Target platform (default: linux/amd64)")
    p_build.add_argument("--no-cache", action="store_true", help="Build without Docker cache")
    p_build.add_argument("--push", action="store_true", help="Push to ECR after building")
    p_build.add_argument("--project-root", help="Project root (default: cwd or $PROJECT_ROOT)")
    p_build.set_defaults(func=cmd_build)

    # --- push ---
    p_push = sub.add_parser("push", help="Push Docker image to ECR")
    p_push.add_argument("--tag", help="Image tag (default: git SHA)")
    p_push.set_defaults(func=cmd_push)

    # --- snapshot ---
    p_snap = sub.add_parser("snapshot", help="Manage EBS data snapshots")
    snap_sub = p_snap.add_subparsers(dest="snap_cmd")
    p_snap_ls = snap_sub.add_parser("list", help="List EBS snapshots tagged with ManagedBy=cloud-run")
    p_snap_ls.set_defaults(func=cmd_snapshot_list)
    p_snap_create = snap_sub.add_parser("create", help="Print manual steps to create an EBS data snapshot")
    p_snap_create.set_defaults(func=cmd_snapshot_create)

    args = parser.parse_args()

    if not args.subcmd:
        parser.print_help()
        sys.exit(1)

    if args.subcmd == "volume" and not getattr(args, "vol_cmd", None):
        p_vol.print_help()
        sys.exit(1)

    if args.subcmd == "snapshot" and not getattr(args, "snap_cmd", None):
        p_snap.print_help()
        sys.exit(1)

    args.func(args)


if __name__ == "__main__":
    main()
